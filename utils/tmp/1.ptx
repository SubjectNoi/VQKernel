//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_52
.address_size 64

	// .globl	_Z14decode_shufflePhP6__halfS1_
// _ZZ14decode_shufflePhP6__halfS1_E12codebook_buf has been demoted
// _ZZ14decode_shufflePhP6__halfS1_E6in_buf has been demoted

.visible .entry _Z14decode_shufflePhP6__halfS1_(
	.param .u64 _Z14decode_shufflePhP6__halfS1__param_0,
	.param .u64 _Z14decode_shufflePhP6__halfS1__param_1,
	.param .u64 _Z14decode_shufflePhP6__halfS1__param_2
)
{
	.local .align 8 .b8 	__local_depot0[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<134>;
	.reg .b16 	%rs<38>;
	.reg .b32 	%r<670>;
	.reg .b64 	%rd<311>;
	// demoted variable
	.shared .align 2 .b8 _ZZ14decode_shufflePhP6__halfS1_E12codebook_buf[4096];
	// demoted variable
	.shared .align 1 .b8 _ZZ14decode_shufflePhP6__halfS1_E6in_buf[8192];

	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd101, [_Z14decode_shufflePhP6__halfS1__param_0];
	ld.param.u64 	%rd102, [_Z14decode_shufflePhP6__halfS1__param_1];
	ld.param.u64 	%rd103, [_Z14decode_shufflePhP6__halfS1__param_2];
	cvta.to.global.u64 	%rd1, %rd101;
	cvta.to.global.u64 	%rd2, %rd103;
	add.u64 	%rd3, %SPL, 0;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r36, %r2, 11;
	shl.b32 	%r37, %r1, 3;
	shl.b32 	%r38, %r1, 4;
	mov.u32 	%r39, _ZZ14decode_shufflePhP6__halfS1_E12codebook_buf;
	add.s32 	%r40, %r39, %r38;
	add.s32 	%r41, %r36, %r37;
	cvta.to.global.u64 	%rd105, %rd102;
	mul.wide.u32 	%rd106, %r41, 2;
	add.s64 	%rd107, %rd105, %rd106;
	ld.global.v4.u32 	{%r42, %r43, %r44, %r45}, [%rd107];
	st.shared.v4.u32 	[%r40], {%r42, %r43, %r44, %r45};
	setp.gt.s32 	%p1, %r1, 4095;
	@%p1 bra 	$L__BB0_7;

	shl.b32 	%r3, %r2, 1;
	max.s32 	%r50, %r1, 3840;
	add.s32 	%r51, %r50, 255;
	sub.s32 	%r4, %r51, %r1;
	shr.u32 	%r52, %r4, 8;
	add.s32 	%r53, %r52, 1;
	and.b32  	%r664, %r53, 3;
	setp.eq.s32 	%p2, %r664, 0;
	mov.u32 	%r665, %r1;
	@%p2 bra 	$L__BB0_4;

	shl.b32 	%r54, %r1, 1;
	mov.u32 	%r55, _ZZ14decode_shufflePhP6__halfS1_E6in_buf;
	add.s32 	%r662, %r55, %r54;
	shl.b32 	%r56, %r1, 10;
	add.s32 	%r661, %r56, %r3;
	mov.u32 	%r665, %r1;

$L__BB0_3:
	.pragma "nounroll";
	cvt.u64.u32 	%rd108, %r661;
	add.s64 	%rd109, %rd1, %rd108;
	ld.global.u16 	%rs1, [%rd109];
	st.shared.u16 	[%r662], %rs1;
	add.s32 	%r665, %r665, 256;
	add.s32 	%r662, %r662, 512;
	add.s32 	%r661, %r661, 262144;
	add.s32 	%r664, %r664, -1;
	setp.ne.s32 	%p3, %r664, 0;
	@%p3 bra 	$L__BB0_3;

$L__BB0_4:
	setp.lt.u32 	%p4, %r4, 768;
	@%p4 bra 	$L__BB0_7;

	add.s32 	%r669, %r665, -1024;
	shl.b32 	%r57, %r665, 1;
	mov.u32 	%r58, _ZZ14decode_shufflePhP6__halfS1_E6in_buf;
	add.s32 	%r59, %r58, %r57;
	add.s32 	%r668, %r59, 1024;
	shl.b32 	%r60, %r665, 10;
	add.s32 	%r61, %r60, %r3;
	add.s32 	%r667, %r61, 786432;
	add.s32 	%r666, %r61, 262144;

$L__BB0_6:
	add.s32 	%r62, %r667, -786432;
	cvt.u64.u32 	%rd110, %r62;
	add.s64 	%rd111, %rd1, %rd110;
	ld.global.u16 	%rs2, [%rd111];
	st.shared.u16 	[%r668+-1024], %rs2;
	cvt.u64.u32 	%rd112, %r666;
	add.s64 	%rd113, %rd1, %rd112;
	ld.global.u16 	%rs3, [%rd113];
	st.shared.u16 	[%r668+-512], %rs3;
	add.s32 	%r63, %r667, -262144;
	cvt.u64.u32 	%rd114, %r63;
	add.s64 	%rd115, %rd1, %rd114;
	ld.global.u16 	%rs4, [%rd115];
	st.shared.u16 	[%r668], %rs4;
	cvt.u64.u32 	%rd116, %r667;
	add.s64 	%rd117, %rd1, %rd116;
	ld.global.u16 	%rs5, [%rd117];
	st.shared.u16 	[%r668+512], %rs5;
	add.s32 	%r668, %r668, 2048;
	add.s32 	%r667, %r667, 1048576;
	add.s32 	%r666, %r666, 1048576;
	add.s32 	%r669, %r669, 1024;
	setp.lt.s32 	%p5, %r669, 3072;
	@%p5 bra 	$L__BB0_6;

$L__BB0_7:
	and.b32  	%r64, %r1, 31;
	not.b32 	%r65, %r1;
	mov.u32 	%r66, 31;
	bar.sync 	0;
	mov.u32 	%r67, 1;
	shl.b32 	%r68, %r1, 10;
	and.b32  	%r29, %r68, 1024;
	shl.b32 	%r69, %r1, 2;
	and.b32  	%r70, %r69, 16;
	mov.u32 	%r71, 4;
	bfe.u32 	%r72, %r1, 1, 4;
	and.b32  	%r73, %r72, 12;
	bfe.u32 	%r74, %r1, 1, 1;
	and.b32  	%r75, %r1, -32;
	or.b32  	%r76, %r74, %r75;
	or.b32  	%r77, %r76, %r70;
	or.b32  	%r78, %r77, %r73;
	and.b32  	%r79, %r65, 4;
	cvt.u64.u32 	%rd118, %r79;
	add.s64 	%rd4, %rd3, %rd118;
	shl.b32 	%r80, %r67, %r64;
	and.b32  	%r30, %r80, 1515870810;
	setp.eq.s32 	%p6, %r30, 0;
	shr.u32 	%r31, %r1, 1;
	and.b32  	%r81, %r69, 4;
	shl.b32 	%r82, %r2, 3;
	or.b32  	%r32, %r81, %r82;
	shl.b32 	%r83, %r1, 1;
	and.b32  	%r84, %r83, 2;
	or.b32  	%r85, %r78, %r84;
	mov.u32 	%r86, _ZZ14decode_shufflePhP6__halfS1_E6in_buf;
	add.s32 	%r33, %r86, %r85;
	ld.shared.u8 	%rs6, [%r33];
	mul.wide.u16 	%r87, %rs6, 4;
	or.b32  	%r88, %r87, %r29;
	shl.b32 	%r89, %r88, 1;
	add.s32 	%r91, %r39, %r89;
	ld.shared.u64 	%rd119, [%r91];
	st.local.u64 	[%rd3], %rd119;
	ld.local.u32 	%r92, [%rd4];
	mov.u32 	%r93, -1;
	shfl.sync.bfly.b32 	%r94|%p7, %r92, %r71, %r66, %r93;
	st.local.u32 	[%rd4], %r94;
	ld.local.u64 	%rd279, [%rd3];
	@%p6 bra 	$L__BB0_9;

	// begin inline asm
	mov.b64 {%r95,%r96}, %rd279;
	// end inline asm
	mov.u32 	%r100, 5;
	shfl.sync.bfly.b32 	%r98|%p8, %r96, %r100, %r66, %r93;
	shfl.sync.bfly.b32 	%r97|%p9, %r95, %r100, %r66, %r93;
	// begin inline asm
	mov.b64 %rd279, {%r97,%r98};
	// end inline asm
	st.local.u64 	[%rd3], %rd279;

$L__BB0_9:
	shl.b32 	%r102, %r31, 12;
	add.s32 	%r34, %r32, %r102;
	mul.wide.u32 	%rd122, %r34, 2;
	add.s64 	%rd123, %rd2, %rd122;
	st.global.u64 	[%rd123], %rd279;
	ld.shared.u8 	%rs7, [%r33+256];
	mul.wide.u16 	%r103, %rs7, 4;
	add.s32 	%r104, %r103, %r29;
	shl.b32 	%r105, %r104, 1;
	add.s32 	%r107, %r39, %r105;
	ld.shared.u64 	%rd124, [%r107];
	st.local.u64 	[%rd3], %rd124;
	ld.local.u32 	%r108, [%rd4];
	shfl.sync.bfly.b32 	%r112|%p10, %r108, %r71, %r66, %r93;
	st.local.u32 	[%rd4], %r112;
	ld.local.u64 	%rd280, [%rd3];
	@%p6 bra 	$L__BB0_11;

	// begin inline asm
	mov.b64 {%r113,%r114}, %rd280;
	// end inline asm
	mov.u32 	%r117, 31;
	mov.u32 	%r118, 5;
	mov.u32 	%r119, -1;
	shfl.sync.bfly.b32 	%r116|%p12, %r114, %r118, %r117, %r119;
	shfl.sync.bfly.b32 	%r115|%p13, %r113, %r118, %r117, %r119;
	// begin inline asm
	mov.b64 %rd280, {%r115,%r116};
	// end inline asm
	st.local.u64 	[%rd3], %rd280;

$L__BB0_11:
	add.s32 	%r120, %r34, 524288;
	mul.wide.u32 	%rd127, %r120, 2;
	add.s64 	%rd128, %rd2, %rd127;
	st.global.u64 	[%rd128], %rd280;
	ld.shared.u8 	%rs8, [%r33+512];
	mul.wide.u16 	%r121, %rs8, 4;
	add.s32 	%r122, %r121, %r29;
	shl.b32 	%r123, %r122, 1;
	add.s32 	%r125, %r39, %r123;
	ld.shared.u64 	%rd129, [%r125];
	st.local.u64 	[%rd3], %rd129;
	ld.local.u32 	%r126, [%rd4];
	mov.u32 	%r127, 31;
	mov.u32 	%r128, 4;
	mov.u32 	%r129, -1;
	shfl.sync.bfly.b32 	%r130|%p14, %r126, %r128, %r127, %r129;
	st.local.u32 	[%rd4], %r130;
	ld.local.u64 	%rd281, [%rd3];
	@%p6 bra 	$L__BB0_13;

	// begin inline asm
	mov.b64 {%r131,%r132}, %rd281;
	// end inline asm
	mov.u32 	%r136, 5;
	shfl.sync.bfly.b32 	%r134|%p16, %r132, %r136, %r127, %r129;
	shfl.sync.bfly.b32 	%r133|%p17, %r131, %r136, %r127, %r129;
	// begin inline asm
	mov.b64 %rd281, {%r133,%r134};
	// end inline asm
	st.local.u64 	[%rd3], %rd281;

$L__BB0_13:
	add.s32 	%r138, %r34, 1048576;
	mul.wide.u32 	%rd132, %r138, 2;
	add.s64 	%rd133, %rd2, %rd132;
	st.global.u64 	[%rd133], %rd281;
	ld.shared.u8 	%rs9, [%r33+768];
	mul.wide.u16 	%r139, %rs9, 4;
	add.s32 	%r140, %r139, %r29;
	shl.b32 	%r141, %r140, 1;
	add.s32 	%r143, %r39, %r141;
	ld.shared.u64 	%rd134, [%r143];
	st.local.u64 	[%rd3], %rd134;
	ld.local.u32 	%r144, [%rd4];
	shfl.sync.bfly.b32 	%r148|%p18, %r144, %r128, %r127, %r129;
	st.local.u32 	[%rd4], %r148;
	ld.local.u64 	%rd282, [%rd3];
	@%p6 bra 	$L__BB0_15;

	// begin inline asm
	mov.b64 {%r149,%r150}, %rd282;
	// end inline asm
	mov.u32 	%r153, 31;
	mov.u32 	%r154, 5;
	mov.u32 	%r155, -1;
	shfl.sync.bfly.b32 	%r152|%p20, %r150, %r154, %r153, %r155;
	shfl.sync.bfly.b32 	%r151|%p21, %r149, %r154, %r153, %r155;
	// begin inline asm
	mov.b64 %rd282, {%r151,%r152};
	// end inline asm
	st.local.u64 	[%rd3], %rd282;

$L__BB0_15:
	add.s32 	%r156, %r34, 1572864;
	mul.wide.u32 	%rd137, %r156, 2;
	add.s64 	%rd138, %rd2, %rd137;
	st.global.u64 	[%rd138], %rd282;
	ld.shared.u8 	%rs10, [%r33+1024];
	mul.wide.u16 	%r157, %rs10, 4;
	add.s32 	%r158, %r157, %r29;
	shl.b32 	%r159, %r158, 1;
	add.s32 	%r161, %r39, %r159;
	ld.shared.u64 	%rd139, [%r161];
	st.local.u64 	[%rd3], %rd139;
	ld.local.u32 	%r162, [%rd4];
	mov.u32 	%r163, 31;
	mov.u32 	%r164, 4;
	mov.u32 	%r165, -1;
	shfl.sync.bfly.b32 	%r166|%p22, %r162, %r164, %r163, %r165;
	st.local.u32 	[%rd4], %r166;
	ld.local.u64 	%rd283, [%rd3];
	@%p6 bra 	$L__BB0_17;

	// begin inline asm
	mov.b64 {%r167,%r168}, %rd283;
	// end inline asm
	mov.u32 	%r172, 5;
	shfl.sync.bfly.b32 	%r170|%p24, %r168, %r172, %r163, %r165;
	shfl.sync.bfly.b32 	%r169|%p25, %r167, %r172, %r163, %r165;
	// begin inline asm
	mov.b64 %rd283, {%r169,%r170};
	// end inline asm
	st.local.u64 	[%rd3], %rd283;

$L__BB0_17:
	add.s32 	%r174, %r34, 2097152;
	mul.wide.u32 	%rd142, %r174, 2;
	add.s64 	%rd143, %rd2, %rd142;
	st.global.u64 	[%rd143], %rd283;
	ld.shared.u8 	%rs11, [%r33+1280];
	mul.wide.u16 	%r175, %rs11, 4;
	add.s32 	%r176, %r175, %r29;
	shl.b32 	%r177, %r176, 1;
	add.s32 	%r179, %r39, %r177;
	ld.shared.u64 	%rd144, [%r179];
	st.local.u64 	[%rd3], %rd144;
	ld.local.u32 	%r180, [%rd4];
	shfl.sync.bfly.b32 	%r184|%p26, %r180, %r164, %r163, %r165;
	st.local.u32 	[%rd4], %r184;
	ld.local.u64 	%rd284, [%rd3];
	@%p6 bra 	$L__BB0_19;

	// begin inline asm
	mov.b64 {%r185,%r186}, %rd284;
	// end inline asm
	mov.u32 	%r189, 31;
	mov.u32 	%r190, 5;
	mov.u32 	%r191, -1;
	shfl.sync.bfly.b32 	%r188|%p28, %r186, %r190, %r189, %r191;
	shfl.sync.bfly.b32 	%r187|%p29, %r185, %r190, %r189, %r191;
	// begin inline asm
	mov.b64 %rd284, {%r187,%r188};
	// end inline asm
	st.local.u64 	[%rd3], %rd284;

$L__BB0_19:
	add.s32 	%r192, %r34, 2621440;
	mul.wide.u32 	%rd147, %r192, 2;
	add.s64 	%rd148, %rd2, %rd147;
	st.global.u64 	[%rd148], %rd284;
	ld.shared.u8 	%rs12, [%r33+1536];
	mul.wide.u16 	%r193, %rs12, 4;
	add.s32 	%r194, %r193, %r29;
	shl.b32 	%r195, %r194, 1;
	add.s32 	%r197, %r39, %r195;
	ld.shared.u64 	%rd149, [%r197];
	st.local.u64 	[%rd3], %rd149;
	ld.local.u32 	%r198, [%rd4];
	mov.u32 	%r199, 31;
	mov.u32 	%r200, 4;
	mov.u32 	%r201, -1;
	shfl.sync.bfly.b32 	%r202|%p30, %r198, %r200, %r199, %r201;
	st.local.u32 	[%rd4], %r202;
	ld.local.u64 	%rd285, [%rd3];
	@%p6 bra 	$L__BB0_21;

	// begin inline asm
	mov.b64 {%r203,%r204}, %rd285;
	// end inline asm
	mov.u32 	%r208, 5;
	shfl.sync.bfly.b32 	%r206|%p32, %r204, %r208, %r199, %r201;
	shfl.sync.bfly.b32 	%r205|%p33, %r203, %r208, %r199, %r201;
	// begin inline asm
	mov.b64 %rd285, {%r205,%r206};
	// end inline asm
	st.local.u64 	[%rd3], %rd285;

$L__BB0_21:
	add.s32 	%r210, %r34, 3145728;
	mul.wide.u32 	%rd152, %r210, 2;
	add.s64 	%rd153, %rd2, %rd152;
	st.global.u64 	[%rd153], %rd285;
	ld.shared.u8 	%rs13, [%r33+1792];
	mul.wide.u16 	%r211, %rs13, 4;
	add.s32 	%r212, %r211, %r29;
	shl.b32 	%r213, %r212, 1;
	add.s32 	%r215, %r39, %r213;
	ld.shared.u64 	%rd154, [%r215];
	st.local.u64 	[%rd3], %rd154;
	ld.local.u32 	%r216, [%rd4];
	shfl.sync.bfly.b32 	%r220|%p34, %r216, %r200, %r199, %r201;
	st.local.u32 	[%rd4], %r220;
	ld.local.u64 	%rd286, [%rd3];
	@%p6 bra 	$L__BB0_23;

	// begin inline asm
	mov.b64 {%r221,%r222}, %rd286;
	// end inline asm
	mov.u32 	%r225, 31;
	mov.u32 	%r226, 5;
	mov.u32 	%r227, -1;
	shfl.sync.bfly.b32 	%r224|%p36, %r222, %r226, %r225, %r227;
	shfl.sync.bfly.b32 	%r223|%p37, %r221, %r226, %r225, %r227;
	// begin inline asm
	mov.b64 %rd286, {%r223,%r224};
	// end inline asm
	st.local.u64 	[%rd3], %rd286;

$L__BB0_23:
	add.s32 	%r228, %r34, 3670016;
	mul.wide.u32 	%rd157, %r228, 2;
	add.s64 	%rd158, %rd2, %rd157;
	st.global.u64 	[%rd158], %rd286;
	ld.shared.u8 	%rs14, [%r33+2048];
	mul.wide.u16 	%r229, %rs14, 4;
	add.s32 	%r230, %r229, %r29;
	shl.b32 	%r231, %r230, 1;
	add.s32 	%r233, %r39, %r231;
	ld.shared.u64 	%rd159, [%r233];
	st.local.u64 	[%rd3], %rd159;
	ld.local.u32 	%r234, [%rd4];
	mov.u32 	%r235, 31;
	mov.u32 	%r236, 4;
	mov.u32 	%r237, -1;
	shfl.sync.bfly.b32 	%r238|%p38, %r234, %r236, %r235, %r237;
	st.local.u32 	[%rd4], %r238;
	ld.local.u64 	%rd287, [%rd3];
	@%p6 bra 	$L__BB0_25;

	// begin inline asm
	mov.b64 {%r239,%r240}, %rd287;
	// end inline asm
	mov.u32 	%r244, 5;
	shfl.sync.bfly.b32 	%r242|%p40, %r240, %r244, %r235, %r237;
	shfl.sync.bfly.b32 	%r241|%p41, %r239, %r244, %r235, %r237;
	// begin inline asm
	mov.b64 %rd287, {%r241,%r242};
	// end inline asm
	st.local.u64 	[%rd3], %rd287;

$L__BB0_25:
	add.s32 	%r246, %r34, 4194304;
	mul.wide.u32 	%rd162, %r246, 2;
	add.s64 	%rd163, %rd2, %rd162;
	st.global.u64 	[%rd163], %rd287;
	ld.shared.u8 	%rs15, [%r33+2304];
	mul.wide.u16 	%r247, %rs15, 4;
	add.s32 	%r248, %r247, %r29;
	shl.b32 	%r249, %r248, 1;
	add.s32 	%r251, %r39, %r249;
	ld.shared.u64 	%rd164, [%r251];
	st.local.u64 	[%rd3], %rd164;
	ld.local.u32 	%r252, [%rd4];
	shfl.sync.bfly.b32 	%r256|%p42, %r252, %r236, %r235, %r237;
	st.local.u32 	[%rd4], %r256;
	ld.local.u64 	%rd288, [%rd3];
	@%p6 bra 	$L__BB0_27;

	// begin inline asm
	mov.b64 {%r257,%r258}, %rd288;
	// end inline asm
	mov.u32 	%r261, 31;
	mov.u32 	%r262, 5;
	mov.u32 	%r263, -1;
	shfl.sync.bfly.b32 	%r260|%p44, %r258, %r262, %r261, %r263;
	shfl.sync.bfly.b32 	%r259|%p45, %r257, %r262, %r261, %r263;
	// begin inline asm
	mov.b64 %rd288, {%r259,%r260};
	// end inline asm
	st.local.u64 	[%rd3], %rd288;

$L__BB0_27:
	add.s32 	%r264, %r34, 4718592;
	mul.wide.u32 	%rd167, %r264, 2;
	add.s64 	%rd168, %rd2, %rd167;
	st.global.u64 	[%rd168], %rd288;
	ld.shared.u8 	%rs16, [%r33+2560];
	mul.wide.u16 	%r265, %rs16, 4;
	add.s32 	%r266, %r265, %r29;
	shl.b32 	%r267, %r266, 1;
	add.s32 	%r269, %r39, %r267;
	ld.shared.u64 	%rd169, [%r269];
	st.local.u64 	[%rd3], %rd169;
	ld.local.u32 	%r270, [%rd4];
	mov.u32 	%r271, 31;
	mov.u32 	%r272, 4;
	mov.u32 	%r273, -1;
	shfl.sync.bfly.b32 	%r274|%p46, %r270, %r272, %r271, %r273;
	st.local.u32 	[%rd4], %r274;
	ld.local.u64 	%rd289, [%rd3];
	@%p6 bra 	$L__BB0_29;

	// begin inline asm
	mov.b64 {%r275,%r276}, %rd289;
	// end inline asm
	mov.u32 	%r280, 5;
	shfl.sync.bfly.b32 	%r278|%p48, %r276, %r280, %r271, %r273;
	shfl.sync.bfly.b32 	%r277|%p49, %r275, %r280, %r271, %r273;
	// begin inline asm
	mov.b64 %rd289, {%r277,%r278};
	// end inline asm
	st.local.u64 	[%rd3], %rd289;

$L__BB0_29:
	add.s32 	%r282, %r34, 5242880;
	mul.wide.u32 	%rd172, %r282, 2;
	add.s64 	%rd173, %rd2, %rd172;
	st.global.u64 	[%rd173], %rd289;
	ld.shared.u8 	%rs17, [%r33+2816];
	mul.wide.u16 	%r283, %rs17, 4;
	add.s32 	%r284, %r283, %r29;
	shl.b32 	%r285, %r284, 1;
	add.s32 	%r287, %r39, %r285;
	ld.shared.u64 	%rd174, [%r287];
	st.local.u64 	[%rd3], %rd174;
	ld.local.u32 	%r288, [%rd4];
	shfl.sync.bfly.b32 	%r292|%p50, %r288, %r272, %r271, %r273;
	st.local.u32 	[%rd4], %r292;
	ld.local.u64 	%rd290, [%rd3];
	@%p6 bra 	$L__BB0_31;

	// begin inline asm
	mov.b64 {%r293,%r294}, %rd290;
	// end inline asm
	mov.u32 	%r297, 31;
	mov.u32 	%r298, 5;
	mov.u32 	%r299, -1;
	shfl.sync.bfly.b32 	%r296|%p52, %r294, %r298, %r297, %r299;
	shfl.sync.bfly.b32 	%r295|%p53, %r293, %r298, %r297, %r299;
	// begin inline asm
	mov.b64 %rd290, {%r295,%r296};
	// end inline asm
	st.local.u64 	[%rd3], %rd290;

$L__BB0_31:
	add.s32 	%r300, %r34, 5767168;
	mul.wide.u32 	%rd177, %r300, 2;
	add.s64 	%rd178, %rd2, %rd177;
	st.global.u64 	[%rd178], %rd290;
	ld.shared.u8 	%rs18, [%r33+3072];
	mul.wide.u16 	%r301, %rs18, 4;
	add.s32 	%r302, %r301, %r29;
	shl.b32 	%r303, %r302, 1;
	add.s32 	%r305, %r39, %r303;
	ld.shared.u64 	%rd179, [%r305];
	st.local.u64 	[%rd3], %rd179;
	ld.local.u32 	%r306, [%rd4];
	mov.u32 	%r307, 31;
	mov.u32 	%r308, 4;
	mov.u32 	%r309, -1;
	shfl.sync.bfly.b32 	%r310|%p54, %r306, %r308, %r307, %r309;
	st.local.u32 	[%rd4], %r310;
	ld.local.u64 	%rd291, [%rd3];
	@%p6 bra 	$L__BB0_33;

	// begin inline asm
	mov.b64 {%r311,%r312}, %rd291;
	// end inline asm
	mov.u32 	%r316, 5;
	shfl.sync.bfly.b32 	%r314|%p56, %r312, %r316, %r307, %r309;
	shfl.sync.bfly.b32 	%r313|%p57, %r311, %r316, %r307, %r309;
	// begin inline asm
	mov.b64 %rd291, {%r313,%r314};
	// end inline asm
	st.local.u64 	[%rd3], %rd291;

$L__BB0_33:
	add.s32 	%r318, %r34, 6291456;
	mul.wide.u32 	%rd182, %r318, 2;
	add.s64 	%rd183, %rd2, %rd182;
	st.global.u64 	[%rd183], %rd291;
	ld.shared.u8 	%rs19, [%r33+3328];
	mul.wide.u16 	%r319, %rs19, 4;
	add.s32 	%r320, %r319, %r29;
	shl.b32 	%r321, %r320, 1;
	add.s32 	%r323, %r39, %r321;
	ld.shared.u64 	%rd184, [%r323];
	st.local.u64 	[%rd3], %rd184;
	ld.local.u32 	%r324, [%rd4];
	shfl.sync.bfly.b32 	%r328|%p58, %r324, %r308, %r307, %r309;
	st.local.u32 	[%rd4], %r328;
	ld.local.u64 	%rd292, [%rd3];
	@%p6 bra 	$L__BB0_35;

	// begin inline asm
	mov.b64 {%r329,%r330}, %rd292;
	// end inline asm
	mov.u32 	%r333, 31;
	mov.u32 	%r334, 5;
	mov.u32 	%r335, -1;
	shfl.sync.bfly.b32 	%r332|%p60, %r330, %r334, %r333, %r335;
	shfl.sync.bfly.b32 	%r331|%p61, %r329, %r334, %r333, %r335;
	// begin inline asm
	mov.b64 %rd292, {%r331,%r332};
	// end inline asm
	st.local.u64 	[%rd3], %rd292;

$L__BB0_35:
	add.s32 	%r336, %r34, 6815744;
	mul.wide.u32 	%rd187, %r336, 2;
	add.s64 	%rd188, %rd2, %rd187;
	st.global.u64 	[%rd188], %rd292;
	ld.shared.u8 	%rs20, [%r33+3584];
	mul.wide.u16 	%r337, %rs20, 4;
	add.s32 	%r338, %r337, %r29;
	shl.b32 	%r339, %r338, 1;
	add.s32 	%r341, %r39, %r339;
	ld.shared.u64 	%rd189, [%r341];
	st.local.u64 	[%rd3], %rd189;
	ld.local.u32 	%r342, [%rd4];
	mov.u32 	%r343, 31;
	mov.u32 	%r344, 4;
	mov.u32 	%r345, -1;
	shfl.sync.bfly.b32 	%r346|%p62, %r342, %r344, %r343, %r345;
	st.local.u32 	[%rd4], %r346;
	ld.local.u64 	%rd293, [%rd3];
	@%p6 bra 	$L__BB0_37;

	// begin inline asm
	mov.b64 {%r347,%r348}, %rd293;
	// end inline asm
	mov.u32 	%r352, 5;
	shfl.sync.bfly.b32 	%r350|%p64, %r348, %r352, %r343, %r345;
	shfl.sync.bfly.b32 	%r349|%p65, %r347, %r352, %r343, %r345;
	// begin inline asm
	mov.b64 %rd293, {%r349,%r350};
	// end inline asm
	st.local.u64 	[%rd3], %rd293;

$L__BB0_37:
	add.s32 	%r354, %r34, 7340032;
	mul.wide.u32 	%rd192, %r354, 2;
	add.s64 	%rd193, %rd2, %rd192;
	st.global.u64 	[%rd193], %rd293;
	ld.shared.u8 	%rs21, [%r33+3840];
	mul.wide.u16 	%r355, %rs21, 4;
	add.s32 	%r356, %r355, %r29;
	shl.b32 	%r357, %r356, 1;
	add.s32 	%r359, %r39, %r357;
	ld.shared.u64 	%rd194, [%r359];
	st.local.u64 	[%rd3], %rd194;
	ld.local.u32 	%r360, [%rd4];
	shfl.sync.bfly.b32 	%r364|%p66, %r360, %r344, %r343, %r345;
	st.local.u32 	[%rd4], %r364;
	ld.local.u64 	%rd294, [%rd3];
	@%p6 bra 	$L__BB0_39;

	// begin inline asm
	mov.b64 {%r365,%r366}, %rd294;
	// end inline asm
	mov.u32 	%r369, 31;
	mov.u32 	%r370, 5;
	mov.u32 	%r371, -1;
	shfl.sync.bfly.b32 	%r368|%p68, %r366, %r370, %r369, %r371;
	shfl.sync.bfly.b32 	%r367|%p69, %r365, %r370, %r369, %r371;
	// begin inline asm
	mov.b64 %rd294, {%r367,%r368};
	// end inline asm
	st.local.u64 	[%rd3], %rd294;

$L__BB0_39:
	add.s32 	%r372, %r34, 7864320;
	mul.wide.u32 	%rd197, %r372, 2;
	add.s64 	%rd198, %rd2, %rd197;
	st.global.u64 	[%rd198], %rd294;
	ld.shared.u8 	%rs22, [%r33+4096];
	mul.wide.u16 	%r373, %rs22, 4;
	add.s32 	%r374, %r373, %r29;
	shl.b32 	%r375, %r374, 1;
	add.s32 	%r377, %r39, %r375;
	ld.shared.u64 	%rd199, [%r377];
	st.local.u64 	[%rd3], %rd199;
	ld.local.u32 	%r378, [%rd4];
	mov.u32 	%r379, 31;
	mov.u32 	%r380, 4;
	mov.u32 	%r381, -1;
	shfl.sync.bfly.b32 	%r382|%p70, %r378, %r380, %r379, %r381;
	st.local.u32 	[%rd4], %r382;
	ld.local.u64 	%rd295, [%rd3];
	@%p6 bra 	$L__BB0_41;

	// begin inline asm
	mov.b64 {%r383,%r384}, %rd295;
	// end inline asm
	mov.u32 	%r388, 5;
	shfl.sync.bfly.b32 	%r386|%p72, %r384, %r388, %r379, %r381;
	shfl.sync.bfly.b32 	%r385|%p73, %r383, %r388, %r379, %r381;
	// begin inline asm
	mov.b64 %rd295, {%r385,%r386};
	// end inline asm
	st.local.u64 	[%rd3], %rd295;

$L__BB0_41:
	add.s32 	%r390, %r34, 8388608;
	mul.wide.u32 	%rd202, %r390, 2;
	add.s64 	%rd203, %rd2, %rd202;
	st.global.u64 	[%rd203], %rd295;
	ld.shared.u8 	%rs23, [%r33+4352];
	mul.wide.u16 	%r391, %rs23, 4;
	add.s32 	%r392, %r391, %r29;
	shl.b32 	%r393, %r392, 1;
	add.s32 	%r395, %r39, %r393;
	ld.shared.u64 	%rd204, [%r395];
	st.local.u64 	[%rd3], %rd204;
	ld.local.u32 	%r396, [%rd4];
	shfl.sync.bfly.b32 	%r400|%p74, %r396, %r380, %r379, %r381;
	st.local.u32 	[%rd4], %r400;
	ld.local.u64 	%rd296, [%rd3];
	@%p6 bra 	$L__BB0_43;

	// begin inline asm
	mov.b64 {%r401,%r402}, %rd296;
	// end inline asm
	mov.u32 	%r405, 31;
	mov.u32 	%r406, 5;
	mov.u32 	%r407, -1;
	shfl.sync.bfly.b32 	%r404|%p76, %r402, %r406, %r405, %r407;
	shfl.sync.bfly.b32 	%r403|%p77, %r401, %r406, %r405, %r407;
	// begin inline asm
	mov.b64 %rd296, {%r403,%r404};
	// end inline asm
	st.local.u64 	[%rd3], %rd296;

$L__BB0_43:
	add.s32 	%r408, %r34, 8912896;
	mul.wide.u32 	%rd207, %r408, 2;
	add.s64 	%rd208, %rd2, %rd207;
	st.global.u64 	[%rd208], %rd296;
	ld.shared.u8 	%rs24, [%r33+4608];
	mul.wide.u16 	%r409, %rs24, 4;
	add.s32 	%r410, %r409, %r29;
	shl.b32 	%r411, %r410, 1;
	add.s32 	%r413, %r39, %r411;
	ld.shared.u64 	%rd209, [%r413];
	st.local.u64 	[%rd3], %rd209;
	ld.local.u32 	%r414, [%rd4];
	mov.u32 	%r415, 31;
	mov.u32 	%r416, 4;
	mov.u32 	%r417, -1;
	shfl.sync.bfly.b32 	%r418|%p78, %r414, %r416, %r415, %r417;
	st.local.u32 	[%rd4], %r418;
	ld.local.u64 	%rd297, [%rd3];
	@%p6 bra 	$L__BB0_45;

	// begin inline asm
	mov.b64 {%r419,%r420}, %rd297;
	// end inline asm
	mov.u32 	%r424, 5;
	shfl.sync.bfly.b32 	%r422|%p80, %r420, %r424, %r415, %r417;
	shfl.sync.bfly.b32 	%r421|%p81, %r419, %r424, %r415, %r417;
	// begin inline asm
	mov.b64 %rd297, {%r421,%r422};
	// end inline asm
	st.local.u64 	[%rd3], %rd297;

$L__BB0_45:
	add.s32 	%r426, %r34, 9437184;
	mul.wide.u32 	%rd212, %r426, 2;
	add.s64 	%rd213, %rd2, %rd212;
	st.global.u64 	[%rd213], %rd297;
	ld.shared.u8 	%rs25, [%r33+4864];
	mul.wide.u16 	%r427, %rs25, 4;
	add.s32 	%r428, %r427, %r29;
	shl.b32 	%r429, %r428, 1;
	add.s32 	%r431, %r39, %r429;
	ld.shared.u64 	%rd214, [%r431];
	st.local.u64 	[%rd3], %rd214;
	ld.local.u32 	%r432, [%rd4];
	shfl.sync.bfly.b32 	%r436|%p82, %r432, %r416, %r415, %r417;
	st.local.u32 	[%rd4], %r436;
	ld.local.u64 	%rd298, [%rd3];
	@%p6 bra 	$L__BB0_47;

	// begin inline asm
	mov.b64 {%r437,%r438}, %rd298;
	// end inline asm
	mov.u32 	%r441, 31;
	mov.u32 	%r442, 5;
	mov.u32 	%r443, -1;
	shfl.sync.bfly.b32 	%r440|%p84, %r438, %r442, %r441, %r443;
	shfl.sync.bfly.b32 	%r439|%p85, %r437, %r442, %r441, %r443;
	// begin inline asm
	mov.b64 %rd298, {%r439,%r440};
	// end inline asm
	st.local.u64 	[%rd3], %rd298;

$L__BB0_47:
	add.s32 	%r444, %r34, 9961472;
	mul.wide.u32 	%rd217, %r444, 2;
	add.s64 	%rd218, %rd2, %rd217;
	st.global.u64 	[%rd218], %rd298;
	ld.shared.u8 	%rs26, [%r33+5120];
	mul.wide.u16 	%r445, %rs26, 4;
	add.s32 	%r446, %r445, %r29;
	shl.b32 	%r447, %r446, 1;
	add.s32 	%r449, %r39, %r447;
	ld.shared.u64 	%rd219, [%r449];
	st.local.u64 	[%rd3], %rd219;
	ld.local.u32 	%r450, [%rd4];
	mov.u32 	%r451, 31;
	mov.u32 	%r452, 4;
	mov.u32 	%r453, -1;
	shfl.sync.bfly.b32 	%r454|%p86, %r450, %r452, %r451, %r453;
	st.local.u32 	[%rd4], %r454;
	ld.local.u64 	%rd299, [%rd3];
	@%p6 bra 	$L__BB0_49;

	// begin inline asm
	mov.b64 {%r455,%r456}, %rd299;
	// end inline asm
	mov.u32 	%r460, 5;
	shfl.sync.bfly.b32 	%r458|%p88, %r456, %r460, %r451, %r453;
	shfl.sync.bfly.b32 	%r457|%p89, %r455, %r460, %r451, %r453;
	// begin inline asm
	mov.b64 %rd299, {%r457,%r458};
	// end inline asm
	st.local.u64 	[%rd3], %rd299;

$L__BB0_49:
	add.s32 	%r462, %r34, 10485760;
	mul.wide.u32 	%rd222, %r462, 2;
	add.s64 	%rd223, %rd2, %rd222;
	st.global.u64 	[%rd223], %rd299;
	ld.shared.u8 	%rs27, [%r33+5376];
	mul.wide.u16 	%r463, %rs27, 4;
	add.s32 	%r464, %r463, %r29;
	shl.b32 	%r465, %r464, 1;
	add.s32 	%r467, %r39, %r465;
	ld.shared.u64 	%rd224, [%r467];
	st.local.u64 	[%rd3], %rd224;
	ld.local.u32 	%r468, [%rd4];
	shfl.sync.bfly.b32 	%r472|%p90, %r468, %r452, %r451, %r453;
	st.local.u32 	[%rd4], %r472;
	ld.local.u64 	%rd300, [%rd3];
	@%p6 bra 	$L__BB0_51;

	// begin inline asm
	mov.b64 {%r473,%r474}, %rd300;
	// end inline asm
	mov.u32 	%r477, 31;
	mov.u32 	%r478, 5;
	mov.u32 	%r479, -1;
	shfl.sync.bfly.b32 	%r476|%p92, %r474, %r478, %r477, %r479;
	shfl.sync.bfly.b32 	%r475|%p93, %r473, %r478, %r477, %r479;
	// begin inline asm
	mov.b64 %rd300, {%r475,%r476};
	// end inline asm
	st.local.u64 	[%rd3], %rd300;

$L__BB0_51:
	add.s32 	%r480, %r34, 11010048;
	mul.wide.u32 	%rd227, %r480, 2;
	add.s64 	%rd228, %rd2, %rd227;
	st.global.u64 	[%rd228], %rd300;
	ld.shared.u8 	%rs28, [%r33+5632];
	mul.wide.u16 	%r481, %rs28, 4;
	add.s32 	%r482, %r481, %r29;
	shl.b32 	%r483, %r482, 1;
	add.s32 	%r485, %r39, %r483;
	ld.shared.u64 	%rd229, [%r485];
	st.local.u64 	[%rd3], %rd229;
	ld.local.u32 	%r486, [%rd4];
	mov.u32 	%r487, 31;
	mov.u32 	%r488, 4;
	mov.u32 	%r489, -1;
	shfl.sync.bfly.b32 	%r490|%p94, %r486, %r488, %r487, %r489;
	st.local.u32 	[%rd4], %r490;
	ld.local.u64 	%rd301, [%rd3];
	@%p6 bra 	$L__BB0_53;

	// begin inline asm
	mov.b64 {%r491,%r492}, %rd301;
	// end inline asm
	mov.u32 	%r496, 5;
	shfl.sync.bfly.b32 	%r494|%p96, %r492, %r496, %r487, %r489;
	shfl.sync.bfly.b32 	%r493|%p97, %r491, %r496, %r487, %r489;
	// begin inline asm
	mov.b64 %rd301, {%r493,%r494};
	// end inline asm
	st.local.u64 	[%rd3], %rd301;

$L__BB0_53:
	add.s32 	%r498, %r34, 11534336;
	mul.wide.u32 	%rd232, %r498, 2;
	add.s64 	%rd233, %rd2, %rd232;
	st.global.u64 	[%rd233], %rd301;
	ld.shared.u8 	%rs29, [%r33+5888];
	mul.wide.u16 	%r499, %rs29, 4;
	add.s32 	%r500, %r499, %r29;
	shl.b32 	%r501, %r500, 1;
	add.s32 	%r503, %r39, %r501;
	ld.shared.u64 	%rd234, [%r503];
	st.local.u64 	[%rd3], %rd234;
	ld.local.u32 	%r504, [%rd4];
	shfl.sync.bfly.b32 	%r508|%p98, %r504, %r488, %r487, %r489;
	st.local.u32 	[%rd4], %r508;
	ld.local.u64 	%rd302, [%rd3];
	@%p6 bra 	$L__BB0_55;

	// begin inline asm
	mov.b64 {%r509,%r510}, %rd302;
	// end inline asm
	mov.u32 	%r513, 31;
	mov.u32 	%r514, 5;
	mov.u32 	%r515, -1;
	shfl.sync.bfly.b32 	%r512|%p100, %r510, %r514, %r513, %r515;
	shfl.sync.bfly.b32 	%r511|%p101, %r509, %r514, %r513, %r515;
	// begin inline asm
	mov.b64 %rd302, {%r511,%r512};
	// end inline asm
	st.local.u64 	[%rd3], %rd302;

$L__BB0_55:
	add.s32 	%r516, %r34, 12058624;
	mul.wide.u32 	%rd237, %r516, 2;
	add.s64 	%rd238, %rd2, %rd237;
	st.global.u64 	[%rd238], %rd302;
	ld.shared.u8 	%rs30, [%r33+6144];
	mul.wide.u16 	%r517, %rs30, 4;
	add.s32 	%r518, %r517, %r29;
	shl.b32 	%r519, %r518, 1;
	add.s32 	%r521, %r39, %r519;
	ld.shared.u64 	%rd239, [%r521];
	st.local.u64 	[%rd3], %rd239;
	ld.local.u32 	%r522, [%rd4];
	mov.u32 	%r523, 31;
	mov.u32 	%r524, 4;
	mov.u32 	%r525, -1;
	shfl.sync.bfly.b32 	%r526|%p102, %r522, %r524, %r523, %r525;
	st.local.u32 	[%rd4], %r526;
	ld.local.u64 	%rd303, [%rd3];
	@%p6 bra 	$L__BB0_57;

	// begin inline asm
	mov.b64 {%r527,%r528}, %rd303;
	// end inline asm
	mov.u32 	%r532, 5;
	shfl.sync.bfly.b32 	%r530|%p104, %r528, %r532, %r523, %r525;
	shfl.sync.bfly.b32 	%r529|%p105, %r527, %r532, %r523, %r525;
	// begin inline asm
	mov.b64 %rd303, {%r529,%r530};
	// end inline asm
	st.local.u64 	[%rd3], %rd303;

$L__BB0_57:
	add.s32 	%r534, %r34, 12582912;
	mul.wide.u32 	%rd242, %r534, 2;
	add.s64 	%rd243, %rd2, %rd242;
	st.global.u64 	[%rd243], %rd303;
	ld.shared.u8 	%rs31, [%r33+6400];
	mul.wide.u16 	%r535, %rs31, 4;
	add.s32 	%r536, %r535, %r29;
	shl.b32 	%r537, %r536, 1;
	add.s32 	%r539, %r39, %r537;
	ld.shared.u64 	%rd244, [%r539];
	st.local.u64 	[%rd3], %rd244;
	ld.local.u32 	%r540, [%rd4];
	shfl.sync.bfly.b32 	%r544|%p106, %r540, %r524, %r523, %r525;
	st.local.u32 	[%rd4], %r544;
	ld.local.u64 	%rd304, [%rd3];
	@%p6 bra 	$L__BB0_59;

	// begin inline asm
	mov.b64 {%r545,%r546}, %rd304;
	// end inline asm
	mov.u32 	%r549, 31;
	mov.u32 	%r550, 5;
	mov.u32 	%r551, -1;
	shfl.sync.bfly.b32 	%r548|%p108, %r546, %r550, %r549, %r551;
	shfl.sync.bfly.b32 	%r547|%p109, %r545, %r550, %r549, %r551;
	// begin inline asm
	mov.b64 %rd304, {%r547,%r548};
	// end inline asm
	st.local.u64 	[%rd3], %rd304;

$L__BB0_59:
	add.s32 	%r552, %r34, 13107200;
	mul.wide.u32 	%rd247, %r552, 2;
	add.s64 	%rd248, %rd2, %rd247;
	st.global.u64 	[%rd248], %rd304;
	ld.shared.u8 	%rs32, [%r33+6656];
	mul.wide.u16 	%r553, %rs32, 4;
	add.s32 	%r554, %r553, %r29;
	shl.b32 	%r555, %r554, 1;
	add.s32 	%r557, %r39, %r555;
	ld.shared.u64 	%rd249, [%r557];
	st.local.u64 	[%rd3], %rd249;
	ld.local.u32 	%r558, [%rd4];
	mov.u32 	%r559, 31;
	mov.u32 	%r560, 4;
	mov.u32 	%r561, -1;
	shfl.sync.bfly.b32 	%r562|%p110, %r558, %r560, %r559, %r561;
	st.local.u32 	[%rd4], %r562;
	ld.local.u64 	%rd305, [%rd3];
	@%p6 bra 	$L__BB0_61;

	// begin inline asm
	mov.b64 {%r563,%r564}, %rd305;
	// end inline asm
	mov.u32 	%r568, 5;
	shfl.sync.bfly.b32 	%r566|%p112, %r564, %r568, %r559, %r561;
	shfl.sync.bfly.b32 	%r565|%p113, %r563, %r568, %r559, %r561;
	// begin inline asm
	mov.b64 %rd305, {%r565,%r566};
	// end inline asm
	st.local.u64 	[%rd3], %rd305;

$L__BB0_61:
	add.s32 	%r570, %r34, 13631488;
	mul.wide.u32 	%rd252, %r570, 2;
	add.s64 	%rd253, %rd2, %rd252;
	st.global.u64 	[%rd253], %rd305;
	ld.shared.u8 	%rs33, [%r33+6912];
	mul.wide.u16 	%r571, %rs33, 4;
	add.s32 	%r572, %r571, %r29;
	shl.b32 	%r573, %r572, 1;
	add.s32 	%r575, %r39, %r573;
	ld.shared.u64 	%rd254, [%r575];
	st.local.u64 	[%rd3], %rd254;
	ld.local.u32 	%r576, [%rd4];
	shfl.sync.bfly.b32 	%r580|%p114, %r576, %r560, %r559, %r561;
	st.local.u32 	[%rd4], %r580;
	ld.local.u64 	%rd306, [%rd3];
	@%p6 bra 	$L__BB0_63;

	// begin inline asm
	mov.b64 {%r581,%r582}, %rd306;
	// end inline asm
	mov.u32 	%r585, 31;
	mov.u32 	%r586, 5;
	mov.u32 	%r587, -1;
	shfl.sync.bfly.b32 	%r584|%p116, %r582, %r586, %r585, %r587;
	shfl.sync.bfly.b32 	%r583|%p117, %r581, %r586, %r585, %r587;
	// begin inline asm
	mov.b64 %rd306, {%r583,%r584};
	// end inline asm
	st.local.u64 	[%rd3], %rd306;

$L__BB0_63:
	add.s32 	%r588, %r34, 14155776;
	mul.wide.u32 	%rd257, %r588, 2;
	add.s64 	%rd258, %rd2, %rd257;
	st.global.u64 	[%rd258], %rd306;
	ld.shared.u8 	%rs34, [%r33+7168];
	mul.wide.u16 	%r589, %rs34, 4;
	add.s32 	%r590, %r589, %r29;
	shl.b32 	%r591, %r590, 1;
	add.s32 	%r593, %r39, %r591;
	ld.shared.u64 	%rd259, [%r593];
	st.local.u64 	[%rd3], %rd259;
	ld.local.u32 	%r594, [%rd4];
	mov.u32 	%r595, 31;
	mov.u32 	%r596, 4;
	mov.u32 	%r597, -1;
	shfl.sync.bfly.b32 	%r598|%p118, %r594, %r596, %r595, %r597;
	st.local.u32 	[%rd4], %r598;
	ld.local.u64 	%rd307, [%rd3];
	@%p6 bra 	$L__BB0_65;

	// begin inline asm
	mov.b64 {%r599,%r600}, %rd307;
	// end inline asm
	mov.u32 	%r604, 5;
	shfl.sync.bfly.b32 	%r602|%p120, %r600, %r604, %r595, %r597;
	shfl.sync.bfly.b32 	%r601|%p121, %r599, %r604, %r595, %r597;
	// begin inline asm
	mov.b64 %rd307, {%r601,%r602};
	// end inline asm
	st.local.u64 	[%rd3], %rd307;

$L__BB0_65:
	add.s32 	%r606, %r34, 14680064;
	mul.wide.u32 	%rd262, %r606, 2;
	add.s64 	%rd263, %rd2, %rd262;
	st.global.u64 	[%rd263], %rd307;
	ld.shared.u8 	%rs35, [%r33+7424];
	mul.wide.u16 	%r607, %rs35, 4;
	add.s32 	%r608, %r607, %r29;
	shl.b32 	%r609, %r608, 1;
	add.s32 	%r611, %r39, %r609;
	ld.shared.u64 	%rd264, [%r611];
	st.local.u64 	[%rd3], %rd264;
	ld.local.u32 	%r612, [%rd4];
	shfl.sync.bfly.b32 	%r616|%p122, %r612, %r596, %r595, %r597;
	st.local.u32 	[%rd4], %r616;
	ld.local.u64 	%rd308, [%rd3];
	@%p6 bra 	$L__BB0_67;

	// begin inline asm
	mov.b64 {%r617,%r618}, %rd308;
	// end inline asm
	mov.u32 	%r621, 31;
	mov.u32 	%r622, 5;
	mov.u32 	%r623, -1;
	shfl.sync.bfly.b32 	%r620|%p124, %r618, %r622, %r621, %r623;
	shfl.sync.bfly.b32 	%r619|%p125, %r617, %r622, %r621, %r623;
	// begin inline asm
	mov.b64 %rd308, {%r619,%r620};
	// end inline asm
	st.local.u64 	[%rd3], %rd308;

$L__BB0_67:
	add.s32 	%r624, %r34, 15204352;
	mul.wide.u32 	%rd267, %r624, 2;
	add.s64 	%rd268, %rd2, %rd267;
	st.global.u64 	[%rd268], %rd308;
	ld.shared.u8 	%rs36, [%r33+7680];
	mul.wide.u16 	%r625, %rs36, 4;
	add.s32 	%r626, %r625, %r29;
	shl.b32 	%r627, %r626, 1;
	add.s32 	%r629, %r39, %r627;
	ld.shared.u64 	%rd269, [%r629];
	st.local.u64 	[%rd3], %rd269;
	ld.local.u32 	%r630, [%rd4];
	mov.u32 	%r631, 31;
	mov.u32 	%r632, 4;
	mov.u32 	%r633, -1;
	shfl.sync.bfly.b32 	%r634|%p126, %r630, %r632, %r631, %r633;
	st.local.u32 	[%rd4], %r634;
	ld.local.u64 	%rd309, [%rd3];
	@%p6 bra 	$L__BB0_69;

	// begin inline asm
	mov.b64 {%r635,%r636}, %rd309;
	// end inline asm
	mov.u32 	%r640, 5;
	shfl.sync.bfly.b32 	%r638|%p128, %r636, %r640, %r631, %r633;
	shfl.sync.bfly.b32 	%r637|%p129, %r635, %r640, %r631, %r633;
	// begin inline asm
	mov.b64 %rd309, {%r637,%r638};
	// end inline asm
	st.local.u64 	[%rd3], %rd309;

$L__BB0_69:
	add.s32 	%r642, %r34, 15728640;
	mul.wide.u32 	%rd272, %r642, 2;
	add.s64 	%rd273, %rd2, %rd272;
	st.global.u64 	[%rd273], %rd309;
	ld.shared.u8 	%rs37, [%r33+7936];
	mul.wide.u16 	%r643, %rs37, 4;
	add.s32 	%r644, %r643, %r29;
	shl.b32 	%r645, %r644, 1;
	add.s32 	%r647, %r39, %r645;
	ld.shared.u64 	%rd274, [%r647];
	st.local.u64 	[%rd3], %rd274;
	ld.local.u32 	%r648, [%rd4];
	shfl.sync.bfly.b32 	%r652|%p130, %r648, %r632, %r631, %r633;
	st.local.u32 	[%rd4], %r652;
	ld.local.u64 	%rd310, [%rd3];
	@%p6 bra 	$L__BB0_71;

	// begin inline asm
	mov.b64 {%r653,%r654}, %rd310;
	// end inline asm
	mov.u32 	%r657, 31;
	mov.u32 	%r658, 5;
	mov.u32 	%r659, -1;
	shfl.sync.bfly.b32 	%r656|%p132, %r654, %r658, %r657, %r659;
	shfl.sync.bfly.b32 	%r655|%p133, %r653, %r658, %r657, %r659;
	// begin inline asm
	mov.b64 %rd310, {%r655,%r656};
	// end inline asm
	st.local.u64 	[%rd3], %rd310;

$L__BB0_71:
	add.s32 	%r660, %r34, 16252928;
	mul.wide.u32 	%rd277, %r660, 2;
	add.s64 	%rd278, %rd2, %rd277;
	st.global.u64 	[%rd278], %rd310;
	ret;

}

